{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motor Imagery Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aeon.datasets import load_from_tsfile\n",
    "import numpy as np # for some mathematical operations\n",
    "def load_data(DATA_PATH,t):\n",
    "    if t=='train':\n",
    "        train_x, train_y = load_from_tsfile(DATA_PATH + \"/MotorImagery/MotorImagery_TRAIN.ts\")\n",
    "        return [train_x,train_y]\n",
    "    elif t=='test':\n",
    "        test_x, test_y = load_from_tsfile(DATA_PATH + \"/MotorImagery/MotorImagery_TEST.ts\")\n",
    "        return [test_x, test_y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test the load_data function\n",
    "[data, data_y] = load_data(\"datasets\",'train')  # 'path', 'test'/'train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First time series:\n",
      "[[  6.59375   6.59375   6.1875  ...  -0.46875  -0.21875  -0.0625 ]\n",
      " [ 13.       12.71875  12.84375 ... -13.3125  -13.09375 -12.84375]\n",
      " [ 11.9375   12.4375   12.96875 ...  -8.75     -9.3125  -10.5    ]\n",
      " ...\n",
      " [-10.125    -9.       -8.8125  ...  20.8125   19.34375  18.09375]\n",
      " [ -6.46875  -5.96875  -5.90625 ...   9.09375   9.9375   10.59375]\n",
      " [ -7.6875   -8.1875   -8.59375 ...  16.09375  15.96875  14.9375 ]] \n",
      "\n",
      "First target:\n",
      "finger \n",
      "\n",
      "Shape of train dataset:\n",
      "(278, 64, 3000)\n"
     ]
    }
   ],
   "source": [
    "print('First time series:')\n",
    "print(data[0],\"\\n\")\n",
    "print('First target:')\n",
    "print(data_y[0],\"\\n\")\n",
    "print('Shape of train dataset:') #(samples, features,timestamps) \n",
    "print(np.shape(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pappe\\AppData\\Local\\anaconda3\\envs\\time_series\\lib\\site-packages\\tslearn\\bases\\bases.py:15: UserWarning: h5py not installed, hdf5 features will not be supported.\n",
      "Install h5py to use hdf5 features: http://docs.h5py.org/\n",
      "  warn(h5py_msg)\n"
     ]
    }
   ],
   "source": [
    "from tslearn.preprocessing import TimeSeriesScalerMeanVariance\n",
    "\n",
    "def standarize(data):\n",
    "    scaler = TimeSeriesScalerMeanVariance(mu=0.0, std=1.0)  # Standardize to mean=0, std=1\n",
    "    scaled_data_3d = scaler.fit_transform(data)\n",
    "    return scaled_data_3d\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: (278, 64, 3000)\n",
      "Reshaped shape: (278, 3000, 64)\n"
     ]
    }
   ],
   "source": [
    "# Transform from (samples, features,timestamps) to (samples, timestamps, features) to apply standarisation\n",
    "reshaped_data = np.transpose(data, (0, 2, 1))\n",
    "print(\"Original shape:\", data.shape)          # (instances, features, timepoints)\n",
    "print(\"Reshaped shape:\", reshaped_data.shape) # (instances, timepoints, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First time series:\n",
      "[[ 0.01036473  1.14424664  0.97613651 ... -1.41133536 -0.85179719\n",
      "  -1.04100093]\n",
      " [ 0.01036473  1.09728199  1.05420102 ... -1.2337847  -0.77437809\n",
      "  -1.11495918]\n",
      " [-0.06236365  1.11815517  1.13714456 ... -1.20419293 -0.7647007\n",
      "  -1.17505026]\n",
      " ...\n",
      " [-1.2539901  -3.24955691 -2.25378255 ...  3.47130764  1.55787228\n",
      "   2.47663847]\n",
      " [-1.20923417 -3.21302885 -2.34160513 ...  3.2395054   1.68851701\n",
      "   2.45814891]\n",
      " [-1.18126172 -3.1712825  -2.52700834 ...  3.04222689  1.79012957\n",
      "   2.30561001]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "scaled_data=standarize(reshaped_data)\n",
    "# scaled_data\n",
    "print('First time series:')\n",
    "print(scaled_data[0],\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(278, 3000, 64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(scaled_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 1: intrinsic metrics before vs. after Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before FS:\n",
    "* Representation Entropy correlation based\n",
    "* Variance\n",
    "* Redundancy Rate RED\n",
    "\n",
    "TO DO:\n",
    "* Information Gain Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Representation Entropy\n",
    "import numpy as np\n",
    "from scipy.linalg import eigh\n",
    "\n",
    "def compute_representation_entropy(data):\n",
    "    \"\"\"\n",
    "    Compute Representation Entropy (RE) of a multivariate dataset.\n",
    "    \n",
    "    Args:\n",
    "        data (numpy.ndarray): 2D-Data with shape (samples, features).\n",
    "    \n",
    "    Returns:\n",
    "        float: Representation Entropy (RE).\n",
    "    \"\"\"\n",
    "    # Step 1: Compute the covariance matrix of the dataset (features x features)\n",
    "    #covariance_matrix = np.cov(data, rowvar=False)  # rowvar=False means variables are columns\n",
    "\n",
    "    block_size=100\n",
    "    data_centered = data - np.mean(data, axis=0)\n",
    "    num_features = data.shape[1]\n",
    "    covariance_matrix = np.zeros((num_features, num_features), dtype=np.float64)\n",
    "\n",
    "    for i in range(0, num_features, block_size):\n",
    "        for j in range(i, num_features, block_size):\n",
    "            block_i = data_centered[:, i:i+block_size]\n",
    "            block_j = data_centered[:, j:j+block_size]\n",
    "            block_cov = np.dot(block_i.T, block_j) / (data.shape[0] - 1)\n",
    "            covariance_matrix[i:i+block_size, j:j+block_size] = block_cov\n",
    "            if i != j:\n",
    "                covariance_matrix[j:j+block_size, i:i+block_size] = block_cov.T\n",
    "\n",
    "\n",
    "    # # Step 2: Compute eigenvalues of the covariance matrix\n",
    "    # eigenvalues = np.linalg.eigvals(covariance_matrix) THIS METHOD WAS REPLACED BC OF INSTABILITY\n",
    "    eigenvalues, eigenvectors = eigh(covariance_matrix)\n",
    "\n",
    "    # Step 3: Normalize the eigenvalues to act as probabilities\n",
    "    eigenvalues_sum = np.sum(eigenvalues)\n",
    "    normalized_eigenvalues = eigenvalues / eigenvalues_sum\n",
    "\n",
    "    # Step 4: Compute Representation Entropy using the formula\n",
    "    representation_entropy = -np.sum(normalized_eigenvalues * np.log(normalized_eigenvalues))\n",
    "\n",
    "    return representation_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(834000, 64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Need to flatten the Time Dimension\n",
    "data_flattened = scaled_data.reshape(-1, scaled_data.shape[2])\n",
    "np.shape(data_flattened)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.291436807583561"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_representation_entropy(data_flattened)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overall variance: 1.0000000000000027\n",
      "Redundancy Rate (Correlation-Based): 0.21865414234438776\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# calculate variance\n",
    "overall_variance = data_flattened.var().mean()\n",
    "print('overall variance:', overall_variance)\n",
    "\n",
    "# Compute the correlation matrix\n",
    "corr_matrix = pd.DataFrame(data_flattened).corr().abs()\n",
    "# Calculate average absolute correlation (excluding the diagonal)\n",
    "avg_corr = (corr_matrix.values.sum() - len(corr_matrix)) / (len(corr_matrix) * (len(corr_matrix) - 1))\n",
    "redundancy_rate = avg_corr\n",
    "print(\"Redundancy Rate (Correlation-Based):\", redundancy_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TEST dataset\n",
    "[TESTdata, TESTdata_y] = load_data(\"datasets\",'test')  # 'path', 'test'/'train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: (278, 64, 3000)\n",
      "Reshaped shape: (278, 3000, 64)\n"
     ]
    }
   ],
   "source": [
    "#Preprocess the TEST data\n",
    "\n",
    "reshaped_TESTdata = np.transpose(TESTdata, (0, 2, 1))\n",
    "print(\"Original shape:\", data.shape)          # (instances, features, timepoints)\n",
    "print(\"Reshaped shape:\", reshaped_data.shape) # (instances, timepoints, features)\n",
    "scaled_TESTdata=standarize(reshaped_TESTdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before Representation entropy:  3.3064268109668387\n",
      "before overall variance: 1.000000000000001\n",
      "before Redundancy Rate (Correlation-Based): 0.16871039936347354\n"
     ]
    }
   ],
   "source": [
    "# Compute all intrinsic metrics again for original data_TEST\n",
    "\n",
    "# Compute representation entropy\n",
    "before_TESTdata_flattened = scaled_TESTdata.reshape(-1, scaled_TESTdata.shape[2])\n",
    "before_representation_entropy = compute_representation_entropy(before_TESTdata_flattened)\n",
    "print('before Representation entropy: ', before_representation_entropy)\n",
    "\n",
    "# calculate variance\n",
    "before_overall_variance = before_TESTdata_flattened.var().mean()\n",
    "print('before overall variance:', before_overall_variance)\n",
    "\n",
    "# Compute the correlation matrix\n",
    "before_corr_matrix = pd.DataFrame(before_TESTdata_flattened).corr().abs()\n",
    "# Calculate average absolute correlation (excluding the diagonal)\n",
    "before_avg_corr = (before_corr_matrix.values.sum() - len(before_corr_matrix)) / (len(before_corr_matrix) * (len(before_corr_matrix) - 1))\n",
    "before_redundancy_rate = before_avg_corr\n",
    "print(\"before Redundancy Rate (Correlation-Based):\", before_redundancy_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300000, 64)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(before_TESTdata_flattened)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection 1: CLeVer Hybrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import pdist\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "def compute_dcpcs(data, variance_threshold=0.8):\n",
    "    \"\"\"\n",
    "    Compute Descriptive Common Principal Components (DCPCs) for a set of multivariate time series.\n",
    "\n",
    "    Parameters:\n",
    "    - data: ndarray of shape (num_samples, num_features, time_steps), the time-series dataset.\n",
    "    - variance_threshold: float, the cumulative variance explained to determine the number of PCs.\n",
    "\n",
    "    Returns:\n",
    "    - dcpc_loadings: ndarray of shape (num_dcpcs, num_features), loadings of the DCPCs.\n",
    "    \"\"\"\n",
    "    num_samples, num_features, time_steps = data.shape\n",
    "\n",
    "    # Step 1: Compute PCs for each MTS item\n",
    "    pc_matrices = []  # Store PC loadings for each sample\n",
    "    for sample in range(num_samples):\n",
    "        # Compute correlation matrix for each sample\n",
    "        correlation_matrix = np.corrcoef(data[sample])\n",
    "        # Perform PCA on the correlation matrix\n",
    "        pca = PCA()\n",
    "        pca.fit(correlation_matrix)\n",
    "        pc_matrices.append(pca.components_[:pca.n_components_])\n",
    "\n",
    "    # Step 2: Compute DCPCs across all samples using SVD\n",
    "    all_pc_matrices = np.concatenate(pc_matrices, axis=0)  # Combine PC loadings from all samples\n",
    "    dcpc_covariance = all_pc_matrices.T @ all_pc_matrices\n",
    "    eigvals, eigvecs = np.linalg.eigh(dcpc_covariance)\n",
    "    sorted_indices = np.argsort(eigvals)[::-1]\n",
    "    eigvecs = eigvecs[:, sorted_indices]  # Sort eigenvectors by eigenvalues\n",
    "\n",
    "    # Select DCPCs explaining the desired variance threshold\n",
    "    cumulative_variance = np.cumsum(eigvals[sorted_indices]) / np.sum(eigvals)\n",
    "    num_dcpcs = np.searchsorted(cumulative_variance, variance_threshold) + 1\n",
    "    dcpc_loadings = eigvecs[:, :num_dcpcs].T\n",
    "\n",
    "    return dcpc_loadings\n",
    "\n",
    "def cluster_features(dcpc_loadings, n_clusters):\n",
    "    \"\"\"\n",
    "    Cluster features based on their DCPC loadings using K-means.\n",
    "\n",
    "    Parameters:\n",
    "    - dcpc_loadings: ndarray of shape (num_dcpcs, num_features), loadings of the DCPCs.\n",
    "    - n_clusters: int, number of clusters.\n",
    "\n",
    "    Returns:\n",
    "    - cluster_labels: ndarray of shape (num_features,), cluster assignments for each feature.\n",
    "    \"\"\"\n",
    "    kmeans = KMeans(n_clusters=n_clusters, n_init=10, random_state=42)\n",
    "    cluster_labels = kmeans.fit_predict(dcpc_loadings.T)\n",
    "    return cluster_labels\n",
    "\n",
    "def rank_features(dcpc_loadings, cluster_labels):\n",
    "    \"\"\"\n",
    "    Rank features within each cluster based on their contribution to DCPCs.\n",
    "\n",
    "    Parameters:\n",
    "    - dcpc_loadings: ndarray of shape (num_dcpcs, num_features), loadings of the DCPCs.\n",
    "    - cluster_labels: ndarray of shape (num_features,), cluster assignments for each feature.\n",
    "\n",
    "    Returns:\n",
    "    - ranked_features: dict, keys are cluster labels, values are ranked feature indices.\n",
    "    \"\"\"\n",
    "    ranked_features = {}\n",
    "    for cluster in np.unique(cluster_labels):\n",
    "        cluster_indices = np.where(cluster_labels == cluster)[0]\n",
    "        cluster_loadings = dcpc_loadings[:, cluster_indices]\n",
    "        scores = np.linalg.norm(cluster_loadings, axis=0)  # L2 norm of loadings\n",
    "        ranking = cluster_indices[np.argsort(scores)[::-1]]  # Sort by descending contribution\n",
    "        ranked_features[cluster] = ranking\n",
    "    return ranked_features\n",
    "\n",
    "def select_top_features(ranked_features, top_n=1):\n",
    "    \"\"\"\n",
    "    Select top-ranked features from each cluster.\n",
    "\n",
    "    Parameters:\n",
    "    - ranked_features: dict, keys are cluster labels, values are ranked feature indices.\n",
    "    - top_n: int, number of features to select from each cluster.\n",
    "\n",
    "    Returns:\n",
    "    - selected_features: list, indices of selected features.\n",
    "    \"\"\"\n",
    "    selected_features = []\n",
    "    for features in ranked_features.values():\n",
    "        selected_features.extend(features[:top_n])\n",
    "    return selected_features\n",
    "\n",
    "def clever_hybrid(data, variance_threshold=0.8, n_clusters=None, top_n=1):\n",
    "    \"\"\"\n",
    "    Perform feature selection using the CLeVer-Hybrid algorithm.\n",
    "\n",
    "    Parameters:\n",
    "    - data: ndarray of shape (num_samples, num_features, time_steps), the time-series dataset.\n",
    "    - variance_threshold: float, variance threshold for selecting DCPCs.\n",
    "    - n_clusters: int, number of clusters (if None, sqrt of num_features is used).\n",
    "    - top_n: int, number of features to select from each cluster.\n",
    "\n",
    "    Returns:\n",
    "    - selected_features: list, indices of selected features.\n",
    "    \"\"\"\n",
    "    num_samples, num_features, _ = data.shape\n",
    "    if n_clusters is None:\n",
    "        n_clusters = int(np.sqrt(num_features))\n",
    "\n",
    "    # Step 1: Compute DCPCs\n",
    "    dcpc_loadings = compute_dcpcs(data, variance_threshold)\n",
    "\n",
    "    # Step 2: Cluster features based on DCPC loadings\n",
    "    cluster_labels = cluster_features(dcpc_loadings, n_clusters)\n",
    "\n",
    "    # Step 3: Rank features within clusters\n",
    "    ranked_features = rank_features(dcpc_loadings, cluster_labels)\n",
    "\n",
    "    # Step 4: Select top features from each cluster\n",
    "    selected_features = select_top_features(ranked_features, top_n)\n",
    "\n",
    "    return selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(278, 64, 3000)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_data_ift= np.transpose(scaled_data, (0, 2, 1))\n",
    "np.shape(scaled_data_ift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features CLeVer Hybrid:  [55, 35, 54, 62, 27, 52, 17, 60, 24, 7, 37, 56, 44, 0, 57]\n"
     ]
    }
   ],
   "source": [
    "selected_features_CLeVerH=clever_hybrid(scaled_data_ift, n_clusters=15,top_n=1)\n",
    "print(\"Selected features CLeVer Hybrid: \", selected_features_CLeVerH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 3000, 64)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(scaled_TESTdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered TEST dataset shape:  (100, 3000, 15)\n"
     ]
    }
   ],
   "source": [
    "# Filter the TEST Dataset according to the selected features from CLeVer\n",
    "selected_TESTdata_CLeVerH = scaled_TESTdata[:, :, selected_features_CLeVerH]\n",
    "print('Filtered TEST dataset shape: ', np.shape(selected_TESTdata_CLeVerH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLEVER Hybrid Representation entropy:  2.3197363756658502\n",
      "CLEVER Hybrid overall variance: 1.0000000000000007\n",
      "CLEVER Hybrid Redundancy Rate (Correlation-Based): 0.18181327379886564\n"
     ]
    }
   ],
   "source": [
    "# Compute all intrinsic metrics again for selected_TESTdata_CLeVerH\n",
    "\n",
    "# Compute representation entropy\n",
    "CLEVERH_TESTdata_flattened = selected_TESTdata_CLeVerH.reshape(-1, selected_TESTdata_CLeVerH.shape[2])\n",
    "CLEVERH_representation_entropy = compute_representation_entropy(CLEVERH_TESTdata_flattened)\n",
    "print('CLEVER Hybrid Representation entropy: ', CLEVERH_representation_entropy)\n",
    "\n",
    "# calculate variance\n",
    "CLEVERH_overall_variance = CLEVERH_TESTdata_flattened.var().mean()\n",
    "print('CLEVER Hybrid overall variance:', CLEVERH_overall_variance)\n",
    "\n",
    "# Compute the correlation matrix\n",
    "CLEVERH_corr_matrix = pd.DataFrame(CLEVERH_TESTdata_flattened).corr().abs()\n",
    "# Calculate average absolute correlation (excluding the diagonal)\n",
    "CLEVERH_avg_corr = (CLEVERH_corr_matrix.values.sum() - len(CLEVERH_corr_matrix)) / (len(CLEVERH_corr_matrix) * (len(CLEVERH_corr_matrix) - 1))\n",
    "CLEVERH_redundancy_rate = CLEVERH_avg_corr\n",
    "print(\"CLEVER Hybrid Redundancy Rate (Correlation-Based):\", CLEVERH_redundancy_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection 2: CLeVer Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "def clever_cluster(data, n_clusters=None, variance_threshold=0.8):\n",
    "    \"\"\"\n",
    "    CLeVer-Cluster implementation.\n",
    "\n",
    "    Parameters:\n",
    "    - data: np.array of shape (samples, features, time_steps)\n",
    "    - n_clusters: Number of feature clusters (if None, heuristic is used)\n",
    "    - variance_threshold: Minimum variance explained by selected PCs\n",
    "\n",
    "    Returns:\n",
    "    - selected_features: List of representative feature indices for each cluster\n",
    "    \"\"\"\n",
    "    num_samples, num_features, time_steps = data.shape\n",
    "\n",
    "    # Step 1: Compute DCPC loadings\n",
    "    dcpc_loadings = compute_dcpcs(data, variance_threshold=variance_threshold)  # Shape: (components, features)\n",
    "\n",
    "    # Step 2: Transpose DCPC loadings to cluster features\n",
    "    feature_embeddings = dcpc_loadings.T  # Shape: (features, components)\n",
    "\n",
    "    # Step 3: Determine number of clusters\n",
    "    if n_clusters is None:\n",
    "        n_clusters = int(np.sqrt(num_features))  # Heuristic for cluster count\n",
    "\n",
    "    # Step 4: Perform K-means clustering on DCPC loadings\n",
    "    kmeans = KMeans(n_clusters=n_clusters, n_init=10, random_state=42)\n",
    "    cluster_labels = kmeans.fit_predict(feature_embeddings)\n",
    "\n",
    "    # Step 5: Select representative features (closest to cluster centroids)\n",
    "    selected_features = []\n",
    "    for cluster in range(n_clusters):\n",
    "        cluster_indices = np.where(cluster_labels == cluster)[0]\n",
    "        centroid = kmeans.cluster_centers_[cluster]\n",
    "\n",
    "        # Find the feature closest to the centroid\n",
    "        distances = np.linalg.norm(feature_embeddings[cluster_indices] - centroid, axis=1)\n",
    "        representative_feature = cluster_indices[np.argmin(distances)]\n",
    "        selected_features.append(representative_feature)\n",
    "\n",
    "    return selected_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLeVer Cluster Selected feature indices: [55, 1, 10, 62, 27, 52, 17, 60, 24, 7, 37, 56, 44, 0, 57]\n"
     ]
    }
   ],
   "source": [
    "selected_features_CLeVerC = clever_cluster(scaled_data_ift, n_clusters=15)\n",
    "print(f'CLeVer Cluster Selected feature indices: {selected_features_CLeVerC}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 3000, 64)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(scaled_TESTdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered TEST dataset shape:  (100, 3000, 15)\n"
     ]
    }
   ],
   "source": [
    "# Filter the TEST Dataset according to the selected features from CLeVer\n",
    "selected_TESTdata_CLeVerC = scaled_TESTdata[:, :, selected_features_CLeVerC]\n",
    "print('Filtered TEST dataset shape: ', np.shape(selected_TESTdata_CLeVerC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLEVER Cluster Representation entropy:  2.386785643166536\n",
      "CLEVER Cluster overall variance: 1.0000000000000004\n",
      "CLEVER Cluster Redundancy Rate (Correlation-Based): 0.15827578681799678\n"
     ]
    }
   ],
   "source": [
    "# Compute all intrinsic metrics again for selected_TESTdata_CLeVerC\n",
    "\n",
    "# Compute representation entropy\n",
    "CLEVERC_TESTdata_flattened = selected_TESTdata_CLeVerC.reshape(-1, selected_TESTdata_CLeVerC.shape[2])\n",
    "CLEVERC_representation_entropy = compute_representation_entropy(CLEVERC_TESTdata_flattened)\n",
    "print('CLEVER Cluster Representation entropy: ', CLEVERC_representation_entropy)\n",
    "\n",
    "# calculate variance\n",
    "CLEVERC_overall_variance = CLEVERC_TESTdata_flattened.var().mean()\n",
    "print('CLEVER Cluster overall variance:', CLEVERC_overall_variance)\n",
    "\n",
    "# Compute the correlation matrix\n",
    "CLEVERC_corr_matrix = pd.DataFrame(CLEVERC_TESTdata_flattened).corr().abs()\n",
    "# Calculate average absolute correlation (excluding the diagonal)\n",
    "CLEVERC_avg_corr = (CLEVERC_corr_matrix.values.sum() - len(CLEVERC_corr_matrix)) / (len(CLEVERC_corr_matrix) * (len(CLEVERC_corr_matrix) - 1))\n",
    "CLEVERC_redundancy_rate = CLEVERC_avg_corr\n",
    "print(\"CLEVER Cluster Redundancy Rate (Correlation-Based):\", CLEVERC_redundancy_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection 3: CLeVer Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def clever_ranking(data, num_features_to_select=5, variance_threshold=0.8):\n",
    "    \"\"\"\n",
    "    CLeVer Ranking method for feature selection.\n",
    "\n",
    "    Parameters:\n",
    "    - data: np.array of shape (samples, features, time_steps)\n",
    "    - num_features_to_select: Number of top-ranked features to select\n",
    "    - variance_threshold: Variance threshold for PCA\n",
    "\n",
    "    Returns:\n",
    "    - selected_features: List of indices of the top-ranked features\n",
    "    \"\"\"\n",
    "    # Step 1: Compute DCPC loadings\n",
    "    dcpc_loadings = compute_dcpcs(data,variance_threshold=variance_threshold)\n",
    "\n",
    "    # Step 2: Rank features based on their contribution to the DCPCs\n",
    "    feature_scores = np.linalg.norm(dcpc_loadings, axis=1)  # L2 norm of DCPC loadings\n",
    "    ranked_features = np.argsort(feature_scores)[::-1]  # Sort in descending order\n",
    "\n",
    "    # Step 3: Select top features\n",
    "    selected_features = ranked_features[:num_features_to_select]\n",
    "\n",
    "    return selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLeVer Rank Selected feature indices: [ 9 41  2 45  4 22 30  1 48  3 44 19  7 10 37]\n"
     ]
    }
   ],
   "source": [
    "selected_features_CLeVerR = clever_ranking(scaled_data_ift, num_features_to_select=15)\n",
    "print(f'CLeVer Rank Selected feature indices: {selected_features_CLeVerR}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered TEST dataset shape:  (100, 3000, 15)\n"
     ]
    }
   ],
   "source": [
    "# Filter the TEST Dataset according to the selected features from CLeVer\n",
    "selected_TESTdata_CLeVerR = scaled_TESTdata[:, :, selected_features_CLeVerR]\n",
    "print('Filtered TEST dataset shape: ', np.shape(selected_TESTdata_CLeVerR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLEVER Cluster Representation entropy:  2.458904459993201\n",
      "CLEVER Cluster overall variance: 1.0000000000000004\n",
      "CLEVER Cluster Redundancy Rate (Correlation-Based): 0.15861265696490554\n"
     ]
    }
   ],
   "source": [
    "# Compute all intrinsic metrics again for selected_TESTdata_CLeVerR\n",
    "\n",
    "# Compute representation entropy\n",
    "CLEVERR_TESTdata_flattened = selected_TESTdata_CLeVerR.reshape(-1, selected_TESTdata_CLeVerR.shape[2])\n",
    "CLEVERR_representation_entropy = compute_representation_entropy(CLEVERR_TESTdata_flattened)\n",
    "print('CLEVER Cluster Representation entropy: ', CLEVERR_representation_entropy)\n",
    "\n",
    "# calculate variance\n",
    "CLEVERR_overall_variance = CLEVERR_TESTdata_flattened.var().mean()\n",
    "print('CLEVER Cluster overall variance:', CLEVERR_overall_variance)\n",
    "\n",
    "# Compute the correlation matrix\n",
    "CLEVERR_corr_matrix = pd.DataFrame(CLEVERR_TESTdata_flattened).corr().abs()\n",
    "# Calculate average absolute correlation (excluding the diagonal)\n",
    "CLEVERR_avg_corr = (CLEVERR_corr_matrix.values.sum() - len(CLEVERR_corr_matrix)) / (len(CLEVERR_corr_matrix) * (len(CLEVERR_corr_matrix) - 1))\n",
    "CLEVERR_redundancy_rate = CLEVERR_avg_corr\n",
    "print(\"CLEVER Cluster Redundancy Rate (Correlation-Based):\", CLEVERR_redundancy_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 2: Perform Timeseries-k-Means and evaluate clustering performance UNSUPERVISED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering evaluation Metrics:\n",
    "* Silhouette \n",
    "* Davies-Bouldin Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before FS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DTW Time series clsutering erfolgreich 133 min\n",
    "clustering of training dataset not required, commented out on last line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DTW k-means\n"
     ]
    }
   ],
   "source": [
    "from tslearn.clustering import TimeSeriesKMeans\n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "print(\"DTW k-means\")\n",
    "sdtw_km = TimeSeriesKMeans(n_clusters=2,\n",
    "                           metric=\"dtw\",\n",
    "                           verbose=True,\n",
    "                           random_state=seed)\n",
    "#y_pred = sdtw_km.fit_predict(scaled_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering should be done on the TEST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 3000, 64)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(scaled_TESTdata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "thefit_predict(X, y=None)\n",
    "Fit k-means clustering using X and then predict the closest cluster each time series in X belongs to.\n",
    "\n",
    "Parameters:\n",
    "Xarray-like of shape=(n_ts, sz, d)\n",
    "n_ts: instance, sz:timestamps, d:features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:   22.1s\n",
      "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:   20.7s\n",
      "[Parallel(n_jobs=1)]: Done 199 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:   21.8s\n",
      "[Parallel(n_jobs=1)]: Done 199 tasks      | elapsed:  1.4min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "148539.284 --> "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:   20.1s\n",
      "[Parallel(n_jobs=1)]: Done 199 tasks      | elapsed:  1.4min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76658.675 --> "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:   20.6s\n",
      "[Parallel(n_jobs=1)]: Done 199 tasks      | elapsed:  1.4min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76232.465 --> "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:   19.7s\n",
      "[Parallel(n_jobs=1)]: Done 199 tasks      | elapsed:  1.4min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76151.274 --> "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:   20.3s\n",
      "[Parallel(n_jobs=1)]: Done 199 tasks      | elapsed:  1.4min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76151.274 --> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:   20.8s\n",
      "[Parallel(n_jobs=1)]: Done 199 tasks      | elapsed:  1.4min\n"
     ]
    }
   ],
   "source": [
    "before_y_pred = sdtw_km.fit_predict(scaled_TESTdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,\n",
       "       0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1,\n",
       "       0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1,\n",
       "       1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n",
       "       0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "before_y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before Silhouette Score: 0.31458\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "labels = before_y_pred  # Cluster labels from the model\n",
    "# Flatten the time series for silhouette_score into (instances,timestamps*features)\n",
    "scaled_TESTdata_flattened_instances = scaled_TESTdata.reshape(scaled_TESTdata.shape[0], -1)  \n",
    "\n",
    "before_silhouette_avg = silhouette_score(scaled_TESTdata_flattened_instances, labels, metric='euclidean')\n",
    "print(f\"before Silhouette Score: {before_silhouette_avg:.5f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before Davies-Bouldin Index: 1.28375\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import davies_bouldin_score\n",
    "\n",
    "before_db_index = davies_bouldin_score(scaled_TESTdata_flattened_instances, labels)\n",
    "print(f\"before Davies-Bouldin Index: {before_db_index:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FS1: CLeVer Hybrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 3000, 15)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(selected_TESTdata_CLeVerH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    5.2s\n",
      "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    4.9s\n",
      "[Parallel(n_jobs=1)]: Done 199 tasks      | elapsed:   20.6s\n",
      "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    4.7s\n",
      "[Parallel(n_jobs=1)]: Done 199 tasks      | elapsed:   19.7s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30154.482 --> "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    5.4s\n",
      "[Parallel(n_jobs=1)]: Done 199 tasks      | elapsed:   22.6s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15841.853 --> "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    5.5s\n",
      "[Parallel(n_jobs=1)]: Done 199 tasks      | elapsed:   21.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15718.864 --> "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    5.4s\n",
      "[Parallel(n_jobs=1)]: Done 199 tasks      | elapsed:   20.8s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15718.864 --> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    6.1s\n",
      "[Parallel(n_jobs=1)]: Done 199 tasks      | elapsed:   23.1s\n"
     ]
    }
   ],
   "source": [
    "#Clustering\n",
    "CLEVERH_y_pred = sdtw_km.fit_predict(selected_TESTdata_CLeVerH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLEVER Hybrid Silhouette Score: 0.26295\n",
      "CLEVER Hybrid Davies-Bouldin Index: 1.45908\n"
     ]
    }
   ],
   "source": [
    "# Compute clustering metrics\n",
    "labels_CH = CLEVERH_y_pred  # Cluster labels from the model\n",
    "# Flatten the time series for silhouette_score into (instances,timestamps*features)\n",
    "scaled_TESTdataCH_flattened_instances = selected_TESTdata_CLeVerH.reshape(selected_TESTdata_CLeVerH.shape[0], -1)  \n",
    "\n",
    "CLEVERH_silhouette_avg = silhouette_score(scaled_TESTdataCH_flattened_instances, labels_CH, metric='euclidean')\n",
    "print(f\"CLEVER Hybrid Silhouette Score: {CLEVERH_silhouette_avg:.5f}\")\n",
    "CLEVERH_db_index = davies_bouldin_score(scaled_TESTdataCH_flattened_instances, labels_CH)\n",
    "print(f\"CLEVER Hybrid Davies-Bouldin Index: {CLEVERH_db_index:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FS 2: CLeVer Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    5.6s\n",
      "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    5.1s\n",
      "[Parallel(n_jobs=1)]: Done 199 tasks      | elapsed:   20.8s\n",
      "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    4.9s\n",
      "[Parallel(n_jobs=1)]: Done 199 tasks      | elapsed:   21.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30755.182 --> "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    5.1s\n",
      "[Parallel(n_jobs=1)]: Done 199 tasks      | elapsed:   22.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16976.424 --> "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    5.1s\n",
      "[Parallel(n_jobs=1)]: Done 199 tasks      | elapsed:   21.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16810.813 --> "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    5.3s\n",
      "[Parallel(n_jobs=1)]: Done 199 tasks      | elapsed:   22.4s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16810.813 --> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    5.8s\n",
      "[Parallel(n_jobs=1)]: Done 199 tasks      | elapsed:   23.1s\n"
     ]
    }
   ],
   "source": [
    "#Clustering\n",
    "CLEVERC_y_pred = sdtw_km.fit_predict(selected_TESTdata_CLeVerC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLEVER Cluster Silhouette Score: 0.25703\n",
      "CLEVER Cluster Davies-Bouldin Index: 1.48753\n"
     ]
    }
   ],
   "source": [
    "# Compute clustering metrics\n",
    "labels_CC = CLEVERC_y_pred  # Cluster labels from the model\n",
    "# Flatten the time series for silhouette_score into (instances,timestamps*features)\n",
    "scaled_TESTdataCC_flattened_instances = selected_TESTdata_CLeVerC.reshape(selected_TESTdata_CLeVerC.shape[0], -1)  \n",
    "\n",
    "CLEVERC_silhouette_avg = silhouette_score(scaled_TESTdataCC_flattened_instances, labels_CC, metric='euclidean')\n",
    "print(f\"CLEVER Cluster Silhouette Score: {CLEVERC_silhouette_avg:.5f}\")\n",
    "CLEVERC_db_index = davies_bouldin_score(scaled_TESTdataCC_flattened_instances, labels_CC)\n",
    "print(f\"CLEVER Cluster Davies-Bouldin Index: {CLEVERC_db_index:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FS3: CLeVer Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    5.5s\n",
      "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    5.3s\n",
      "[Parallel(n_jobs=1)]: Done 199 tasks      | elapsed:   20.7s\n",
      "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    5.1s\n",
      "[Parallel(n_jobs=1)]: Done 199 tasks      | elapsed:   21.8s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38056.341 --> "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    5.2s\n",
      "[Parallel(n_jobs=1)]: Done 199 tasks      | elapsed:   21.8s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20396.010 --> "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    6.4s\n",
      "[Parallel(n_jobs=1)]: Done 199 tasks      | elapsed:   25.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20146.008 --> "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    5.9s\n",
      "[Parallel(n_jobs=1)]: Done 199 tasks      | elapsed:   22.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20103.296 --> "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    5.1s\n",
      "[Parallel(n_jobs=1)]: Done 199 tasks      | elapsed:   22.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20068.713 --> "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    5.1s\n",
      "[Parallel(n_jobs=1)]: Done 199 tasks      | elapsed:   21.3s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20068.713 --> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    5.4s\n",
      "[Parallel(n_jobs=1)]: Done 199 tasks      | elapsed:   22.2s\n"
     ]
    }
   ],
   "source": [
    "#Clustering\n",
    "CLEVERR_y_pred = sdtw_km.fit_predict(selected_TESTdata_CLeVerR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLEVER Rank Silhouette Score: 0.30075\n",
      "CLEVER Rank Davies-Bouldin Index: 1.34867\n"
     ]
    }
   ],
   "source": [
    "# Compute clustering metrics\n",
    "labels_CR = CLEVERR_y_pred  # Cluster labels from the model\n",
    "# Flatten the time series for silhouette_score into (instances,timestamps*features)\n",
    "scaled_TESTdataCR_flattened_instances = selected_TESTdata_CLeVerR.reshape(selected_TESTdata_CLeVerR.shape[0], -1)  \n",
    "\n",
    "CLEVERR_silhouette_avg = silhouette_score(scaled_TESTdataCR_flattened_instances, labels_CR, metric='euclidean')\n",
    "print(f\"CLEVER Rank Silhouette Score: {CLEVERR_silhouette_avg:.5f}\")\n",
    "CLEVERR_db_index = davies_bouldin_score(scaled_TESTdataCR_flattened_instances, labels_CR)\n",
    "print(f\"CLEVER Rank Davies-Bouldin Index: {CLEVERR_db_index:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validierung mit clustering accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before FS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare clustering vs labels\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "def clustering_accuracy(true_labels, predicted_labels):\n",
    "    # Create a confusion matrix\n",
    "    cm = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "    # Use the Hungarian algorithm to find the optimal assignment of clusters\n",
    "    row_ind, col_ind = linear_sum_assignment(-cm)  # Maximize the matching (negative to maximize)\n",
    "    \n",
    "    # Calculate accuracy based on optimal matching\n",
    "    accuracy = cm[row_ind, col_ind].sum() / len(true_labels)\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['tongue', 'tongue', 'finger', 'finger', 'finger', 'finger',\n",
       "       'finger', 'finger', 'tongue', 'finger', 'tongue', 'tongue',\n",
       "       'tongue', 'tongue', 'tongue', 'tongue', 'finger', 'finger',\n",
       "       'tongue', 'tongue', 'tongue', 'finger', 'finger', 'tongue',\n",
       "       'tongue', 'finger', 'tongue', 'tongue', 'finger', 'tongue',\n",
       "       'tongue', 'finger', 'tongue', 'finger', 'tongue', 'tongue',\n",
       "       'tongue', 'tongue', 'finger', 'tongue', 'tongue', 'finger',\n",
       "       'tongue', 'finger', 'tongue', 'finger', 'finger', 'tongue',\n",
       "       'tongue', 'finger', 'finger', 'finger', 'tongue', 'tongue',\n",
       "       'finger', 'finger', 'tongue', 'finger', 'tongue', 'finger',\n",
       "       'finger', 'finger', 'finger', 'finger', 'tongue', 'tongue',\n",
       "       'tongue', 'finger', 'tongue', 'finger', 'tongue', 'finger',\n",
       "       'tongue', 'finger', 'finger', 'tongue', 'tongue', 'finger',\n",
       "       'tongue', 'tongue', 'finger', 'tongue', 'finger', 'tongue',\n",
       "       'tongue', 'finger', 'finger', 'finger', 'finger', 'finger',\n",
       "       'finger', 'finger', 'tongue', 'finger', 'finger', 'finger',\n",
       "       'finger', 'tongue', 'tongue', 'tongue'], dtype='<U6')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TESTdata_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,\n",
       "       0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1,\n",
       "       0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1,\n",
       "       1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n",
       "       0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "before_y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_labels_to_numeric(labels):\n",
    "    \"\"\"\n",
    "    Convert string labels to numeric labels based on a predefined mapping.\n",
    "\n",
    "    Args:\n",
    "        labels (list or array): The string labels to be converted.\n",
    "\n",
    "    Returns:\n",
    "        list: Numeric labels.\n",
    "    \"\"\"\n",
    "    # Define the mapping\n",
    "    label_mapping = {\n",
    "        'tongue': 0,\n",
    "        'finger': 1,\n",
    "    }\n",
    "    \n",
    "    # Map labels\n",
    "    numeric_labels = [label_mapping[label] for label in labels]\n",
    "\n",
    "    return numeric_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original labels: ['tongue' 'tongue' 'finger' 'finger' 'finger' 'finger' 'finger' 'finger'\n",
      " 'tongue' 'finger' 'tongue' 'tongue' 'tongue' 'tongue' 'tongue' 'tongue'\n",
      " 'finger' 'finger' 'tongue' 'tongue' 'tongue' 'finger' 'finger' 'tongue'\n",
      " 'tongue' 'finger' 'tongue' 'tongue' 'finger' 'tongue' 'tongue' 'finger'\n",
      " 'tongue' 'finger' 'tongue' 'tongue' 'tongue' 'tongue' 'finger' 'tongue'\n",
      " 'tongue' 'finger' 'tongue' 'finger' 'tongue' 'finger' 'finger' 'tongue'\n",
      " 'tongue' 'finger' 'finger' 'finger' 'tongue' 'tongue' 'finger' 'finger'\n",
      " 'tongue' 'finger' 'tongue' 'finger' 'finger' 'finger' 'finger' 'finger'\n",
      " 'tongue' 'tongue' 'tongue' 'finger' 'tongue' 'finger' 'tongue' 'finger'\n",
      " 'tongue' 'finger' 'finger' 'tongue' 'tongue' 'finger' 'tongue' 'tongue'\n",
      " 'finger' 'tongue' 'finger' 'tongue' 'tongue' 'finger' 'finger' 'finger'\n",
      " 'finger' 'finger' 'finger' 'finger' 'tongue' 'finger' 'finger' 'finger'\n",
      " 'finger' 'tongue' 'tongue' 'tongue']\n",
      "Numeric labels: [0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "numeric_labels = convert_labels_to_numeric(TESTdata_y)\n",
    "print(\"Original labels:\", TESTdata_y)\n",
    "print(\"Numeric labels:\", numeric_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering Accuracy before FS: 0.58\n"
     ]
    }
   ],
   "source": [
    "ClusteringACC_before = clustering_accuracy(numeric_labels,before_y_pred)\n",
    "print(f\"Clustering Accuracy before FS: {ClusteringACC_before:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FS 1: CLeVer Hybrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering Accuracy CLEVER Hybrid: 0.57000\n"
     ]
    }
   ],
   "source": [
    "# compare clustering vs labels\n",
    "ClusteringACC_CLEVERH = clustering_accuracy(numeric_labels,CLEVERH_y_pred)\n",
    "print(f\"Clustering Accuracy CLEVER Hybrid: {ClusteringACC_CLEVERH:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FS 2: CLeVer Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering Accuracy CLEVER Cluster: 0.58000\n"
     ]
    }
   ],
   "source": [
    "ClusteringACC_CLEVERC = clustering_accuracy(numeric_labels,CLEVERC_y_pred)\n",
    "print(f\"Clustering Accuracy CLEVER Cluster: {ClusteringACC_CLEVERC:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FS3: CLeVer Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering Accuracy CLEVER Rank: 0.58000\n"
     ]
    }
   ],
   "source": [
    "ClusteringACC_CLEVERR = clustering_accuracy(numeric_labels,CLEVERR_y_pred)\n",
    "print(f\"Clustering Accuracy CLEVER Rank: {ClusteringACC_CLEVERR:.5f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "time_series",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
